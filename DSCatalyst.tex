\documentclass[a4paper,11pt]{article}
\usepackage[top=2cm,bottom=2cm,left=2cm,right=2cm,asymmetric]{geometry}
\usepackage{url}
\usepackage{paralist}
\usepackage{authblk}
%\usepackage[multiple]{footmisc}
\usepackage[pdftex,colorlinks=true,hyperfootnotes=false]{hyperref}

\title{\vspace{-4em}Digital Science Catalyst Grant 2014}

\author[1]{Tom Crick}
\author[2]{Benjamin A. Hall}
\author[3]{Samin Ishtiaq}
\affil[1]{Department of Computing \& Information Systems, Cardiff Metropolitan University}
\affil[2]{MRC Cancer Unit, University of Cambridge}
\affil[3]{Microsoft Research Cambridge}
\affil[1]{\protect\url{tcrick@cardiffmet.ac.uk}}
% \affil[2]{\protect\url{bh418@mrc-cu.cam.ac.uk}}
% \affil[3]{\protect\url{samin.ishtiaq@microsoft.com}}


\renewcommand\Authands{ and }
\def\UrlBreaks{\do\/\do-}

\date{ }

\begin{document}
\maketitle

% taken from here: http://www.digital-science.com/what-we-do/start-up-investment/catalyst

% To apply, send us a proposal of a max of 1,500 words (with diagrams)
% including the following: 

% a description of the product or innovation
% an explanation of how it would benefit scientific research
% background information, including competitor information
% a timetable for its development
% a budget breakdown and how you would spend the funds


\vspace{-1.5cm}
\subsection*{Overview}

Reproducibility (replication, repeatability) is a basic tenet of good
science. The retraction of numerous results, for disciplines ranging
from climate science to bioscience, has drawn the focus of many
commentators.

These studies are examples of {\emph{digital science}}, a new way of
doing science with fundamentally very different research outcomes:
that of {\emph{algorithms}} and {\emph{models}}. However, despite this
advantage, and alongside ongoing -- and significant -- changes to the
traditional models of academic dissemination and
publication~\cite{deroure:2010,stodden-et-al:2013,fursin+dubach:2014}
and recognition of the important of scientific
software~\cite{goble:2014}, there remain cultural and technical
barriers to both the sharing and reimplementation of
algorithms. Alongside this new way of working, is the simple downside
that it is still not easy to share (use, repeat, compare, contribute)
these artefacts. The difficulty of sharing is partly grounded as a
cultural problem, alongside a wider technical/infrastructural problem.

We believe that an automated notify+reproduce system, which allows
easy reproduction of the results of algorithms running of models, will
significantly improve the efficiency of scientific exploration.  This
Digital Science Catalyst Grant proposal here builds on this belief.

% The reproduction and replication of reported scientific results is a
% hot topic within the academic community. The retraction of numerous
% studies from a wide range of disciplines, from climate science to
% bioscience, has drawn the focus of many commentators, but there exists
% a wider socio-cultural problem that pervades the scientific community.
% Sharing code, data and models often requires extra effort; this is
% currently seen as a significant overhead that may not be worth the
% time investment.
% %
% We believe that automated systems, which allow easy reproduction of results, offer the
% potential to incentivise a culture change and drive the adoption of
% new techniques to improve the efficiency of scientific exploration. 
% Our propsal here builds on this belief. 


%In
%this paper, we discuss the value of improved access and sharing of the
%two key types of results arising from work done in the computational
%sciences: models and algorithms. 

% \subsection*{Background}
% Two key types of results arise from work done in the computational
% sciences: {\emph{models}}~\cite{crick-et-al_recomp14} and {\emph{algorithms}}~\cite{crick-et-al_wssspe2}. Models represent an
% abstraction of reality, and their behaviour is expected to be reliably
% reproduced even if different algorithms are used. This validation of a
% model's behaviour can be impacted by a number of factors relating to
% the specific techniques used, but similar approaches are expected to
% give broadly the same results.  In contrast, when new algorithms are
% proposed to replace or supplement existing algorithms, they are
% expected to verifiably replicate the results of other algorithms.
% Neither class of result can be assessed in isolation- the development 
% of algorithms requires models which have a known behaviour, whilst
% the assessment of model behaviours may require a knowledge of the 
% specific algorithms used to explore that behaviour. Furthermore, models
% may necessitate algorithm development, and new algorithms may make 
% previously intractable models analysable.

\subsection*{Description}
{\textbf{We propose to develop a prototype open software platform
which will automate the complete testing procedure, from code to
models.}} By developing a cloud-based, centralised service, which
performs automated code compilation, testing and benchmarking, we will
link together published implementations of algorithms and input
models. This will allow the future extension of the prototype to link
together software and data repositories, toolchains, workflows and
outputs, providing a seamless automated infrastructure for the
verification and validation of scientific models and in particular,
performance benchmarks. The program of work will lead the cultural
shift in both the short- and long-term to move to a world in which
computational reproducibility helps researchers achieve their goals,
rather than being perceived as an overhead.

A system as described here has several up-front benefits: it links
papers more closely to their outputs, making external validation
easier and allows interested users to explore unaddressed sets of
models. Critically, it helps researchers to be more productive, rather
than being an overhead on their day-to-day work. In the same way that
tools such as GitHub make collaborating easier while simultaneously
allowing effortless sharing, we hope that we can design and build a
system that is similarly usable for sharing and testing benchmarks
online.

% other examples?
There are already several web services that can do aspects all of this
things (for example, a repository for disseminating the computational
models associated with publications in the social and life
sciences~\cite{rollins-et-al:2014}), so a service that can integrate
most if not all of these features is feasible. Such a service would
then allow algorithms and models to evolve together, and be
reproducible from the outset.

In summary, this proposed new infrastructure, previously highlighted
and discussed by the
authors~\cite{crick-et-al_wssspe2,crick-et-al_recomp2014}, would have
a profound impact on the way that open computational science is
performed, repositioning the role of models, algorithms and benchmarks
and accelerating the research cycle, perhaps truly enabling a ``fourth
paradigm'' of data intensive scientific
discovery~\cite{hey:2009}. Furthermore, it would effect the vital
cultural change by reducing overheads and improving the efficiency of
researchers.

\subsection*{Vision, Objective and Goals}

In the software development world, no one would (should) commit to a
project without first running the smoke tests. You could be clever and
run the tests via the version control system's pre-commit hook. That
way you would never forget to run the tests. All of this can be done,
at scale, on the cloud now. Services such as
Jenkins\footnote{\url{http://jenkins-ci.org/}}, Visual Studio
Online\footnote{\url{http://www.visualstudio.com/en-us/products/what-is-visual-studio-online-vs.aspx}},
etc, schedule the tests to run as soon as you commit. We envisage
moving to a world in which benchmarks become available online, in the
same vein as open access of publications and research data. It seems a
small step to hook these continuous integration (CI) systems up to the
algorithm implementations that are written to run on these benchmarks.

Suppose you have come up with a better algorithm to deal with some of
these benchmarks. You write up the paper on the algorithm but, more
importantly, you also register the implementation of your algorithm at
this open service, as a possible algorithm to run on this benchmark
set. The benchmarks live in distributed git (or similar)
repositories. Some of the servers that house these repositories are CI
servers. Now, when you push a commit to your algorithm, or someone
else pushes a commit to theirs, or when someone else adds a new
benchmark, the service's CI system is triggered. It is also activated
with the addition of a new library, firmware upgrade, API change,
etc. All registered algorithms are run on all registered models, and
the results are published. The CI servers act as an authoritative
source, analogous to the Linux Kernel
Archives\footnote{\url{https://www.kernel.org/}}, of results for these
algorithms running on these benchmarks.

The objective of this proposal is to develop a system which, through
integration with publicly available source code repositories automates
the build, testing and benchmarking of algorithms and benchmarks. The 
system will allow testing models against competing algorithms, and the
addition of new models to the test suite (either manually or from existing
online repositories). The goals are to:

\begin{itemize}
	\item Build a webserver, running a daemon which automatically pulls, and compiles
code from git repositories;
\item Run automated tests defined by the developers on the code;
\item Perform analysis of benchmark sets supplied by both the developer and external
users.
\end{itemize}

This will be achieved over a period of six months, including regular
meetings for the design and requirements of the tool, and will involve
the employment of a dedicated programmer to implement the system.

%The whole premise of this paper is that {\emph{algorithms}}
%(implementations) and {\emph{models}} (benchmarks) are inextricably
%linked. Algorithms are designed for certain types of models; models,
%though created to mimic some physical reality, also serve to stress
%the current known algorithms. An integrated autonomous cloud-based
%service can make this link explicit.


%Note- move/integrate with introduction?



\subsection*{Budget}

The major components of the budget are based around buying out Crick's
time and enabling him to visit Cambridge to work with the rest of the
project team, along with the employment of a dedicated programmer to
implement the system:

\begin{itemize}
\item We have costed six separate week-long visits to Cambridge for
Crick: direct buy-out costs for a week are \pounds 350 (covering seven
hours of teaching); one week's accommodation (seven nights in
Cambridge at c.\pounds 130) \pounds 910; one week's subsistence (seven days at \pounds 25) \pounds 200; round trip travel: \pounds
180; (therefore, one individual trip costs \pounds 1600): \hfill {\textbf{\pounds 9,600}}

\item Programmer to implement the system, in conjuction with the
  project team: \hfill {\textbf{\pounds 5,000}}

\item Travel and subsistence for meetings with key stakeholders (for
  example, GitHub, Microsoft Azure team, Mozilla Science Lab, etc):
  \hfill {\textbf{\pounds 400}}

\item {\textbf{GRANT TOTAL: \hfill \pounds 15,000}}
\end{itemize}

\subsection*{Team Profile}

Dr Tom Crick\footnote{\url{http://drtomcrick.com}} is a Senior Lecturer in
Computing Science at Cardiff Metropolitan University, having completed
his PhD and post-doctoral research at the University of Bath. His
research interests cut across computational science: knowledge
representation and reasoning, intelligent systems, big data analytics,
optimisation and high performance computing.  He is the Nesta Data
Science Fellow, a 2014 Fellow of the Software Sustainability Institute
(EPSRC) and a member of {\emph{HiPEAC}}, the European FP7 Network of
Excellence on High Performance and Embedded Architecture and
Compilation.

Dr Benjamin A. Hall\footnote{\url{http://www.mrc-cu.cam.ac.uk/hall.html}}
is a Royal Society University Research Fellow, developing hybrid and
formal models of carcinogenesis and biological signalling at the MRC
Cancer Unit, University of Cambridge. He previously worked at
Microsoft Research (Cambridge), UCL and the University of Oxford. As
part of his role at Oxford, he was one of two Apple
Laureates, awarded by Apple and the Oxford Supercomputing Centre for
the project {\emph{A biomolecular simulation pipeline}}. Benjamin has an
MBiochem and DPhil from the University of Oxford.

Dr Samin
Ishtiaq\footnote{\url{http://research.microsoft.com/en-us/people/sishtiaq/}}
is Principal Engineer in the Programming Principles and Tools group at
Microsoft Research Cambridge. He currently works on the SLAyer
(Separation Logic-based memory safety for C programs), TERMINATOR
(program termination) and BMA (analysis of gene regulatory networks)
projects. Samin joined MSR in April 2008. Before that (2000-2008), he
worked in CPU modelling and verification at ARM, helping to tape-out
the Cortex A8, Cortex M3 and SC300 processors, and the AMBA bus
protocol checker. Samin has an MEng from Imperial and a PhD in
dependent type theory from Queen Mary.

\bibliographystyle{plain}
\bibliography{DSCatalyst}

\end{document}
