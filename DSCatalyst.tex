\documentclass[a4paper,11pt]{article}
\usepackage[top=2cm,bottom=2cm,left=2cm,right=2cm,asymmetric]{geometry}
\usepackage{url}
\usepackage{paralist}
\usepackage{authblk}
\usepackage[multiple]{footmisc}
\usepackage[pdftex,colorlinks=true,hyperfootnotes=false]{hyperref}

\title{\vspace{-4em}Digital Science Catalyst Grant 2014}

\author[1]{Tom Crick}
\author[2]{Benjamin A. Hall}
\author[3]{Samin Ishtiaq}
\affil[1]{Department of Computing \& Information Systems, Cardiff Metropolitan University}
\affil[2]{MRC Cancer Unit, University of Cambridge}
\affil[3]{Microsoft Research Cambridge}
\affil[1]{\protect\url{tcrick@cardiffmet.ac.uk}}
% \affil[2]{\protect\url{bh418@mrc-cu.cam.ac.uk}}
% \affil[3]{\protect\url{samin.ishtiaq@microsoft.com}}


\renewcommand\Authands{ and }

\date{ }

\begin{document}
\maketitle

% taken from here: http://www.digital-science.com/what-we-do/start-up-investment/catalyst

% To apply, send us a proposal of a max of 1,500 words (with diagrams)
% including the following: 

% a description of the product or innovation
% an explanation of how it would benefit scientific research
% background information, including competitor information
% a timetable for its development
% a budget breakdown and how you would spend the funds

\subsection*{Overview}

The reproduction and replication of reported scientific results is a
hot topic within the academic community. The retraction of numerous
studies from a wide range of disciplines, from climate science to
bioscience, has drawn the focus of many commentators, but there exists
a wider socio-cultural problem that pervades the scientific community.
Sharing code, data and models often requires extra effort; this is
currently seen as a significant overhead that may not be worth the
time investment.
%
We believe that automated systems, which allow easy reproduction of results, offer the
potential to incentivise a culture change and drive the adoption of
new techniques to improve the efficiency of scientific exploration. 
Our proposal here builds on this viewpoint. 


%In
%this paper, we discuss the value of improved access and sharing of the
%two key types of results arising from work done in the computational
%sciences: models and algorithms. 

\subsection*{Background}
Two key types of results arise from work done in the computational
sciences: {\emph{models}}~\cite{crick-et-al_recomp14} and {\emph{algorithms}}~\cite{crick-et-al_wssspe2}. Models represent an
abstraction of reality, and their behaviour is expected to be reliably
reproduced even if different algorithms are used. This validation of a
model's behaviour can be impacted by a number of factors relating to
the specific techniques used, but similar approaches are expected to
give broadly the same results.  In contrast, when new algorithms are
proposed to replace or supplement existing algorithms, they are
expected to verifiably replicate the results of other algorithms.
Neither class of result can be assessed in isolation- the development 
of algorithms requires models which have a known behaviour, whilst
the assessment of model behaviours may require a knowledge of the 
specific algorithms used to explore that behaviour. Furthermore, models
may necessitate algorithm development, and new algorithms may make 
previously intractable models analysable.

Scientific progress depends on the replication of observable results, both 
to confirm previous findings and to extend the findings into new areas. 
However, this can be difficult at present- even in a best case scenario,
where the source code and models are available, external dependencies
may limit reproducibility. More commonly, sharing code, data and models 
requires a significant effort that few are willing to invest in.
%Doing science, so want easy replication. But this is very difficult in the 
%current situation. Even sharing of code, data and models requires significant
%effort that few are willing to invest in. 

\subsection*{Proposal}
To address these issues, we propose the development of a prototype
software platform which will automate the complete testing procedure,
from code to models. By developing a cloud-based, centralised service,
which performs automated code compilation, testing and benchmarking, 
we will link together published implementations of algorithms and 
input models. This will allow the future extension of the prototype
to link together software and data repositories, toolchains, workflows
and outputs, providing a seamless automated infrastructure for the
verification and validation of scientific models and in particular,
performance benchmarks. The program of work will lead the cultural 
shift in both the short- and long-term to move to a world in which
computational reproducibility helps researchers achieve their goals,
rather than being perceived as an overhead.

A system as described here has several up-front benefits: it links
papers more closely to their outputs, making external validation
easier and allows interested users to explore unaddressed sets of
models. Critically, it helps researchers to be more productive, rather
than being an overhead on their day-to-day work. In the same way that
tools such as GitHub make collaborating easier while simultaneously
allowing effortless sharing, we hope that we can design and build a
system that is similarly usable for sharing and testing benchmarks
online.

% other examples?
There are already several web services that nearly do all of this
things (for example, a repository for disseminating the computational
models associated with publications in the social and life
sciences~\cite{rollins-et-al:2014}), so a service that can integrate
most if not all of these features is possible. Such a service would
then allow algorithms and models to evolve together, and be
reproducible from the outset.

In summary, this proposed new infrastructure could have a profound
impact on the way that computational science is performed,
repositioning the role of models, algorithms and benchmarks and
accelerating the research cycle, perhaps truly enabling a ``fourth
paradigm'' of data intensive scientific
discovery~\cite{hey:2009}. Furthermore, it would effect the vital
cultural change by reducing overheads and improving the efficiency of
researchers.

\subsection{Vision, Objective and Goals}

In the software development world, no one would (should) commit to a
project without first running the smoke tests. You could be clever and
run the tests via the version control system's pre-commit hook. That
way you would never forget to run the tests. All of this can be done,
at scale, on the cloud now. Services such as
Jenkins\footnote{\url{http://jenkins-ci.org/}}, Visual Studio
Online\footnote{\url{http://www.visualstudio.com/en-us/products/what-is-visual-studio-online-vs.aspx}},
etc, schedule the tests to run as soon as you commit. We envisage
moving to a world in which benchmarks become available online, in the
same vein as open access of publications and research data. It seems a
small step to hook these continuous integration (CI) systems up to the
algorithm implementations that are written to run on these benchmarks.

Suppose you have come up with a better algorithm to deal with some of
these benchmarks. You write up the paper on the algorithm but, more
importantly, you also register the implementation of your algorithm at
this open service, as a possible algorithm to run on this benchmark
set. The benchmarks live in distributed git (or similar)
repositories. Some of the servers that house these repositories are CI
servers. Now, when you push a commit to your algorithm, or someone
else pushes a commit to theirs, or when someone else adds a new
benchmark, the service's CI system is triggered. It is also activated
with the addition of a new library, firmware upgrade, API change,
etc. All registered algorithms are run on all registered models, and
the results are published. The CI servers act as an authoritative
source, analogous to the Linux Kernel
Archives\footnote{\url{https://www.kernel.org/}}, of results for these
algorithms running on these benchmarks.

The objective of this proposal is to develop a system which, through
integration with publically available source code repositories automates
the build, testing and benchmarking of algorithms and benchmarks. The 
system will allow tesing models against competing algorithms, and the
addition of new models to the test suite (either manually or from existing
online repositories). The goals are to:

\begin{itemize}
	\item Build a webserver, running a daemon which automatically pulls, and compiles
code from git repositories
\item Run automated tests defined by the developers on the code
\item Perform analysis of benchmark sets supplied by both the developer and external
users
\end{itemize}

This will be achieved over a period of 6 months, including regular meetings
for the design of the tool, and will involve the employment of a dedicated 
programmer to implement the system.

%The whole premise of this paper is that {\emph{algorithms}}
%(implementations) and {\emph{models}} (benchmarks) are inextricably
%linked. Algorithms are designed for certain types of models; models,
%though created to mimic some physical reality, also serve to stress
%the current known algorithms. An integrated autonomous cloud-based
%service can make this link explicit.


%Note- move/integrate with introduction?



\subsection*{Budget}

Tom's direct costs are \pounds 49.58 per hour, so say 7 hours a week
buyout = c. \pounds 350

One week's hotel: 7 nights at \pounds 130 = \pounds 910

One week's subsistence: 7 days at \pounds 25 = \pounds 175

Round trip travel: \pounds 180

One trip = c.Â£1600

{\textbf{So, five(?) trips = \pounds 8000}}

{\textbf{Programmer = \pounds 5000}}

What other costs?

Trips to see stakeholders (who are they, where are they? People like Mozilla Science Labs, github). 

\subsection*{Team Profile}

% should we each have a brief personal biog here, who we are and what
% we do? 
%BH: Does it ask for one? Also, how much is TMI?

Tom is ace...

Benjamin A Hall is a Royal Society University Research Fellow, developing hybrid and formal models of carcinogenesis and biological signalling at the MRC Cancer Unit, University of Cambridge. He previously worked at Microsoft Research (Cambridge), UCL and the University of Oxford. As part of his role at the University of Oxford, he was one of two Apple Laureates, awarded by Apple and the Oxford Supercomputing Centre for the project "A biomolecular simulation pipeline". Benjamin has an MBiochem and DPhil from the University of Oxford.

Samin Ishtiaq is Principal Engineer in the Programming Principles and Tools group at Microsoft Research Cambridge. He currently works on the SLAyer (Separation Logic-based memory safety for C programs), TERMINATOR (program termination) and BMA (analysis of gene regulatory networks) projects. Samin joined MSR in April 2008. Before that, during 2000-2008, he worked in CPU modeling and verification at ARM, helping to tape-out the Cortex A8, Cortex M3 and SC300 processors, and the AMBA bus protocol checker. Samin has an MEng from Imperial and a PhD in dependent type theory from Queen Mary. 

\bibliographystyle{plain}
\bibliography{DSCatalyst}

\end{document}
